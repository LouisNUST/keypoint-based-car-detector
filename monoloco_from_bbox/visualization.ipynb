{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import torch\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#%matplotlib inline\n",
    "import datetime\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image, ImageFile\n",
    "import cv2\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#%run notebooks/printer\n",
    "%run functions/architectures\n",
    "%run functions/printer\n",
    "\n",
    "import numpy as np\n",
    "from nuscenes.nuscenes import NuScenes\n",
    "from nuscenes.utils import splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of the paths and constants : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_models = [\"/home/bonnesoe/semester_project/monoloco/data/models/old/hyp-monoloco-boxes.pkl\"]  # Trained Monoloco models for the boxes of nuscenes\n",
    "my_models_types = [\"FULL\"]                                                                       # Tag for the dataset (not really important)\n",
    "\n",
    "CAMERAS = ('CAM_FRONT', 'CAM_FRONT_LEFT', 'CAM_FRONT_RIGHT', 'CAM_BACK', 'CAM_BACK_LEFT', 'CAM_BACK_RIGHT')\n",
    "dic_jo = {'train': dict(X=[], Y=[], names=[], kps=[], boxes_3d=[], K=[], clst=defaultdict(lambda: defaultdict(list))),\n",
    "          'val': dict(X=[], Y=[], names=[], kps=[], boxes_3d=[], K=[],clst=defaultdict(lambda: defaultdict(list))),\n",
    "          'test': dict(X=[], Y=[], names=[], kps=[], boxes_3d=[], K=[], clst=defaultdict(lambda: defaultdict(list)))\n",
    "          }\n",
    "dic_names = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "dir_nuscenes='/data/bonnesoeur-data/data/nuscenes/' # Path to the nuscenes dataset\n",
    "dataset = \"nuscenes_teaser\"                         # Type of dataset (nuscenes_teaser or nuscenes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonoLoco:\n",
    "\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    INPUT_SIZE = 9 * 2                   # keypoints (borders and center of the bbox of nuscenes) \n",
    "    LINEAR_SIZE = 256\n",
    "    N_SAMPLES = 100\n",
    "\n",
    "    def __init__(self, model, device=None, n_dropout=0, p_dropout=0.2):\n",
    "\n",
    "        if not device:\n",
    "            self.device = torch.device('cpu')\n",
    "        else:\n",
    "            self.device = device\n",
    "        self.n_dropout = n_dropout\n",
    "        self.epistemic = bool(self.n_dropout > 0)\n",
    "\n",
    "        # if the path is provided load the model parameters\n",
    "        if isinstance(model, str):\n",
    "            model_path = model\n",
    "            self.model = LinearModel(p_dropout=p_dropout, input_size=self.INPUT_SIZE, linear_size=self.LINEAR_SIZE)\n",
    "            self.model.load_state_dict(torch.load(model_path, map_location=lambda storage, loc: storage))\n",
    "\n",
    "        # if the model is directly provided\n",
    "        else:\n",
    "            self.model = model\n",
    "        self.model.eval()  # Default is train\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def forward(self, keypoints, kk):\n",
    "        \"\"\"forward pass of monoloco network\"\"\"\n",
    "        if not keypoints:\n",
    "            return None, None\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = preprocess_monoloco(torch.tensor(keypoints).to(self.device), torch.tensor(kk).to(self.device))\n",
    "            if self.n_dropout > 0:\n",
    "                self.model.dropout.training = True  # Manually reactivate dropout in eval\n",
    "                total_outputs = torch.empty((0, inputs.size()[0])).to(self.device)\n",
    "\n",
    "                for _ in range(self.n_dropout):\n",
    "                    outputs = self.model(inputs)\n",
    "                    outputs = unnormalize_bi(outputs)\n",
    "                    samples = laplace_sampling(outputs, self.N_SAMPLES)\n",
    "                    total_outputs = torch.cat((total_outputs, samples), 0)\n",
    "                varss = total_outputs.std(0)\n",
    "                self.model.dropout.training = False\n",
    "            else:\n",
    "                varss = torch.zeros(inputs.size()[0])\n",
    "\n",
    "            #  Don't use dropout for the mean prediction\n",
    "            outputs = self.model(inputs)\n",
    "            outputs = unnormalize_bi(outputs)\n",
    "        return outputs, varss\n",
    "\n",
    "    @staticmethod\n",
    "    def post_process(outputs, varss, boxes, keypoints, kk, dic_gt=None, iou_min=0.3):\n",
    "        \"\"\"Post process monoloco to output final dictionary with all information for visualizations\"\"\"\n",
    "\n",
    "        dic_out = defaultdict(list)\n",
    "        if outputs is None:\n",
    "            return dic_out\n",
    "\n",
    "        if dic_gt:\n",
    "            boxes_gt, dds_gt = dic_gt['boxes'], dic_gt['dds']\n",
    "            matches = [(idx, idx) for idx, _ in enumerate(boxes)]\n",
    "            print(\"found {} matches with ground-truth\".format(len(matches)))\n",
    "        else:\n",
    "            matches = [(idx, idx) for idx, _ in enumerate(boxes)]  # Replicate boxes\n",
    "\n",
    "        matches = [(idx, idx) for idx, _ in enumerate(boxes)]\n",
    "        \n",
    "        \n",
    "        matches = reorder_matches(matches, boxes, mode='left_right')\n",
    "        uv_shoulders = get_keypoints(keypoints, mode='shoulder')\n",
    "        uv_centers = get_keypoints(keypoints, mode='center')\n",
    "        xy_centers = pixel_to_camera(uv_centers, kk, 1)\n",
    "        \n",
    "                # Match with ground truth if available\n",
    "        for idx, idx_gt in matches:\n",
    "            dd_pred = float(outputs[idx][0])\n",
    "            ale = float(outputs[idx][1])\n",
    "            var_y = float(varss[idx])\n",
    "            dd_real = dds_gt[idx_gt] if dic_gt else dd_pred\n",
    "\n",
    "            kps = keypoints[idx]\n",
    "            box = boxes[idx]\n",
    "            uu_s, vv_s = uv_shoulders.tolist()[idx][0:2]\n",
    "            uu_c, vv_c = uv_centers.tolist()[idx][0:2]\n",
    "            uv_shoulder = [round(uu_s), round(vv_s)]\n",
    "            uv_center = [round(uu_c), round(vv_c)]\n",
    "            xyz_real = xyz_from_distance(dd_real, xy_centers[idx])\n",
    "            xyz_pred = xyz_from_distance(dd_pred, xy_centers[idx])\n",
    "            dic_out['boxes'].append(box)\n",
    "            dic_out['boxes_gt'].append(boxes_gt[idx_gt] if dic_gt else boxes[idx])\n",
    "            dic_out['dds_real'].append(dd_real)\n",
    "            dic_out['dds_pred'].append(dd_pred)\n",
    "            dic_out['stds_ale'].append(ale)\n",
    "            dic_out['stds_epi'].append(var_y)\n",
    "            dic_out['xyz_real'].append(xyz_real.squeeze().tolist())\n",
    "            dic_out['xyz_pred'].append(xyz_pred.squeeze().tolist())\n",
    "            dic_out['uv_kps'].append(kps)\n",
    "            dic_out['uv_centers'].append(uv_center)\n",
    "            dic_out['uv_shoulders'].append(uv_shoulder)\n",
    "\n",
    "        return dic_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_monoloco(keypoints, kk):\n",
    "\n",
    "    \"\"\" Preprocess batches of inputs\n",
    "    keypoints = torch tensors of (m, 3, 14)  or list [3,14]\n",
    "    Outputs =  torch tensors of (m, 34) in meters normalized (z=1) and zero-centered using the center of the box\n",
    "    \"\"\"\n",
    "    if isinstance(keypoints, list):\n",
    "        keypoints = torch.tensor(keypoints)\n",
    "    if isinstance(kk, list):\n",
    "        kk = torch.tensor(kk)\n",
    "    # Projection in normalized image coordinates and zero-center with the center of the bounding box\n",
    "    uv_center = get_keypoints(keypoints, mode='center')\n",
    "    xy1_center = pixel_to_camera(uv_center, kk, 10)\n",
    "    xy1_all = pixel_to_camera(keypoints[:, 0:2, :], kk, 10)\n",
    "    kps_norm = xy1_all - xy1_center.unsqueeze(1)  # (m, 17, 3) - (m, 1, 3)\n",
    "    kps_out = kps_norm[:, :, 0:2].reshape(kps_norm.size()[0], -1)  # no contiguous for view\n",
    "    return kps_out \n",
    "\n",
    "def prepare_kps(kps_in):\n",
    "    \"\"\"Convert from a list of 18 to a list of 3, 9\"\"\"\n",
    "\n",
    "    kps = []\n",
    "    for kp_in in kps_in :\n",
    "        assert len(kp_in) % 2 == 0, \"keypoints expected as a multiple of 2\"\n",
    "        xxs = kp_in[0:][::2]\n",
    "        yys = kp_in[1:][::2]  # from offset 1 every 2\n",
    "        ccs = [1]*len(kp_in[1:][::2])\n",
    "        kps.append([xxs, yys, ccs])\n",
    "    return kps\n",
    "\n",
    "\n",
    "def pixel_to_camera(uv_tensor, kk, z_met):\n",
    "    \"\"\"\n",
    "    Convert a tensor in pixel coordinate to absolute camera coordinates\n",
    "    It accepts lists or torch/numpy tensors of (m, 2) or (m, x, 2)\n",
    "    where x is the number of keypoints\n",
    "    \"\"\"\n",
    "    if isinstance(uv_tensor, (list, np.ndarray)):\n",
    "        uv_tensor = torch.tensor(uv_tensor)\n",
    "    if isinstance(kk, list):\n",
    "        kk = torch.tensor(kk)\n",
    "    if uv_tensor.size()[-1] != 2:\n",
    "        uv_tensor = uv_tensor.permute(0, 2, 1)  # permute to have 2 as last dim to be padded\n",
    "        assert uv_tensor.size()[-1] == 2, \"Tensor size not recognized\"\n",
    "    uv_padded = F.pad(uv_tensor, pad=(0, 1), mode=\"constant\", value=1)  # pad only last-dim below with value 1\n",
    "\n",
    "    kk_1 = torch.inverse(kk)\n",
    "    xyz_met_norm = torch.matmul(uv_padded, kk_1.t())  # More general than torch.mm\n",
    "    xyz_met = xyz_met_norm * z_met\n",
    "\n",
    "    return xyz_met\n",
    "\n",
    "\n",
    "def get_keypoints(keypoints, mode):\n",
    "    \"\"\"\n",
    "    Extract center, shoulder or hip points of a keypoint\n",
    "    Input --> list or torch/numpy tensor [(m, 3, 9) or (3, 9)]\n",
    "    Output --> torch.tensor [(m, 2)]\n",
    "    \"\"\"\n",
    "    if isinstance(keypoints, (list, np.ndarray)):\n",
    "        keypoints = torch.tensor(keypoints)\n",
    "    if len(keypoints.size()) == 2:  # add batch dim\n",
    "        keypoints = keypoints.unsqueeze(0)\n",
    "        \n",
    "    assert len(keypoints.size()) == 3 and keypoints.size()[1] == 3, \"tensor dimensions not recognized\"\n",
    "    assert mode in ['center', 'bottom', 'head', 'shoulder', 'hip', 'ankle']\n",
    "\n",
    "    kps_in = keypoints[:, 0:2, :]  # (m, 2, 9)\n",
    "    if mode == 'center':\n",
    "        kps_max, _ = kps_in.max(2)  # returns value, indices\n",
    "        kps_min, _ = kps_in.min(2)\n",
    "        kps_out = (kps_max - kps_min) / 2 + kps_min   # (m, 2) as keepdims is False\n",
    "\n",
    "    elif mode == 'bottom':  # bottom center for kitti evaluation\n",
    "        kps_max, _ = kps_in.max(2)\n",
    "        kps_min, _ = kps_in.min(2)\n",
    "        kps_out_x = (kps_max[:, 0:1] - kps_min[:, 0:1]) / 2 + kps_min[:, 0:1]\n",
    "        kps_out_y = kps_max[:, 1:2]\n",
    "        kps_out = torch.cat((kps_out_x, kps_out_y), -1)\n",
    "\n",
    "    elif mode == 'head':\n",
    "        kps_out = kps_in[:, :, 0:5].mean(2)\n",
    "\n",
    "    elif mode == 'shoulder':\n",
    "        kps_out = kps_in[:, :, 5:7].mean(2)\n",
    "\n",
    "    elif mode == 'hip':\n",
    "        kps_out = kps_in[:, :, 11:13].mean(2)\n",
    "\n",
    "    elif mode == 'ankle':\n",
    "        kps_out = kps_in[:, :, 15:17].mean(2)\n",
    "\n",
    "    return kps_out  # (m, 2)\n",
    "\n",
    "def xyz_from_distance(distances, xy_centers):\n",
    "    \"\"\"\n",
    "    From distances and normalized image coordinates (z=1), extract the real world position xyz\n",
    "    distances --> tensor (m,1) or (m) or float\n",
    "    xy_centers --> tensor(m,3) or (3)\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(distances, float):\n",
    "        distances = torch.tensor(distances).unsqueeze(0)\n",
    "    if len(distances.size()) == 1:\n",
    "        distances = distances.unsqueeze(1)\n",
    "    if len(xy_centers.size()) == 1:\n",
    "        xy_centers = xy_centers.unsqueeze(0)\n",
    "\n",
    "    assert xy_centers.size()[-1] == 3 and distances.size()[-1] == 1, \"Size of tensor not recognized\"\n",
    "\n",
    "    return xy_centers * distances / torch.sqrt(1 + xy_centers[:, 0:1].pow(2) + xy_centers[:, 1:2].pow(2))\n",
    "\n",
    "\n",
    "def open_image(path_image):\n",
    "    with open(path_image, 'rb') as f:\n",
    "        pil_image = Image.open(f).convert('RGB')\n",
    "        return pil_image\n",
    "    \n",
    "def reorder_matches(matches, boxes, mode='left_rigth'):\n",
    "    \"\"\"\n",
    "    Reorder a list of (idx, idx_gt) matches based on position of the detections in the image\n",
    "    ordered_boxes = (5, 6, 7, 0, 1, 4, 2, 4)\n",
    "    matches = [(0, x), (2,x), (4,x), (3,x), (5,x)]\n",
    "    Output --> [(5, x), (0, x), (3, x), (2, x), (5, x)]\n",
    "    \"\"\"\n",
    "\n",
    "    assert mode == 'left_right'\n",
    "\n",
    "    # Order the boxes based on the left-right position in the image and\n",
    "    ordered_boxes = np.argsort([box[0] for box in boxes])  # indices of boxes ordered from left to right\n",
    "    matches_left = [idx for (idx, _) in matches]\n",
    "\n",
    "    return [matches[matches_left.index(idx_boxes)] for idx_boxes in ordered_boxes if idx_boxes in matches_left]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnormalize_bi(outputs):\n",
    "    \"\"\"Unnormalize relative bi of a nunmpy array\"\"\"\n",
    "\n",
    "    outputs[:, 1] = torch.exp(outputs[:, 1]) * outputs[:, 0]\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageList(torch.utils.data.Dataset):\n",
    "    \"\"\"It defines transformations to apply to images and outputs of the dataloader\"\"\"\n",
    "    def __init__(self, image_paths, scale):\n",
    "        self.image_paths = image_paths\n",
    "        print(img_path)\n",
    "        self.scale = scale\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.image_paths[index]\n",
    "        ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "        with open(image_path, 'rb') as f:\n",
    "            image = Image.open(f).convert('RGB')\n",
    "\n",
    "        if self.scale > 1.01 or self.scale < 0.99:\n",
    "            image = torchvision.transforms.functional.resize(image,\n",
    "                                                             (round(self.scale * image.size[1]),\n",
    "                                                              round(self.scale * image.size[0])),\n",
    "                                                             interpolation=Image.BICUBIC)\n",
    "        # PIL images are not iterables\n",
    "        original_image = torchvision.transforms.functional.to_tensor(image)  # 0-255 --> 0-1\n",
    "        image = image_transform(image)\n",
    "        \n",
    "        \n",
    "\n",
    "        return image_path, original_image, image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def factory_for_gt(im_size, name=None, path_gt=None):\n",
    "    \"\"\"Look for ground-truth annotations file and define calibration matrix based on image size \"\"\"\n",
    "\n",
    "    try:\n",
    "        with open(path_gt, 'r') as f:\n",
    "            dic_names = json.load(f)\n",
    "        print('-' * 120 + \"\\nGround-truth file opened\")\n",
    "    except (FileNotFoundError, TypeError):\n",
    "        print('-' * 120 + \"\\nGround-truth file not found\")\n",
    "        dic_names = {}\n",
    "\n",
    "    try:\n",
    "        kk = dic_names[name]['K']\n",
    "        dic_gt = dic_names[name]\n",
    "        print(\"Matched ground-truth file!\")\n",
    "    except KeyError:\n",
    "        dic_gt = None\n",
    "        x_factor = im_size[0] / 1600\n",
    "        y_factor = im_size[1] / 900\n",
    "        pixel_factor = (x_factor + y_factor) / 2   # TODO remove and check it\n",
    "        if im_size[0] / im_size[1] > 2.5:\n",
    "            kk = [[718.3351, 0., 600.3891], [0., 718.3351, 181.5122], [0., 0., 1.]]  # Kitti calibration\n",
    "        else:\n",
    "            kk = [[1266.4 * pixel_factor, 0., 816.27 * x_factor],\n",
    "                  [0, 1266.4 * pixel_factor, 491.5 * y_factor],\n",
    "                  [0., 0., 1.]]  # nuScenes calibration\n",
    "\n",
    "        print(\"Using a standard calibration matrix...\")\n",
    "\n",
    "    return kk, dic_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def factory_outputs(args, images_outputs, output_path, pifpaf_outputs, dic_out=None, kk=None):\n",
    "    \"\"\"Output json files or images according to the choice\"\"\"\n",
    "\n",
    "    if any((xx in output_types for xx in ['front', 'bird', 'combined'])):\n",
    "        epistemic = False\n",
    "        if args.n_dropout > 0:\n",
    "            epistemic = True\n",
    "\n",
    "        if dic_out['boxes']:  # Only print in case of detections\n",
    "            printer = Printer(images_outputs[1], output_path, kk, output_types=output_types\n",
    "                              , z_max=22, epistemic=epistemic) #default value for zmax \n",
    "            figures, axes = printer.factory_axes()\n",
    "            printer.draw(figures, axes, dic_out, images_outputs[1], draw_box=args.draw_box,\n",
    "                         save=True, show=args.show)\n",
    "\n",
    "    \"\"\"if 'json' in args.output_types:\n",
    "        with open(os.path.join(output_path + '.monoloco.json'), 'w') as ff:\n",
    "            json.dump(dic_out, ff)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction of the relevant informations from the nuscenes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_token(sd_token):\n",
    "\n",
    "        boxes_gt = []\n",
    "        dds = []\n",
    "        boxes_3d = []\n",
    "\n",
    "        keypoints = []\n",
    "        path_im, boxes_obj, kk = nusc.get_sample_data(sd_token, box_vis_level=1)  # At least one corner$\n",
    "\n",
    "    \n",
    "        kk = kk.tolist()\n",
    "        name = os.path.basename(path_im)\n",
    "        for box_obj in boxes_obj:\n",
    "            if box_obj.name[:6] != 'animal':\n",
    "                general_name = box_obj.name.split('.')[0] + '.' + box_obj.name.split('.')[1]\n",
    "            else:\n",
    "                general_name = 'animal'\n",
    "            if general_name in select_categories('car'):\n",
    "                 \n",
    "                keypoint = project_2d(box_obj, kk)\n",
    "                dd = np.linalg.norm(box_obj.center)\n",
    "                bound = prepare_kps([keypoint])[0]\n",
    "                box_gt = [min(bound[0]), min(bound[1]), max(bound[0]), max(bound[1])]\n",
    "                \n",
    "                dds.append(dd)\n",
    "                \n",
    "                \n",
    "                box_3d = box_obj.center.tolist() + box_obj.wlh.tolist()\n",
    "                boxes_3d.append(box_3d)\n",
    "                keypoints.append(keypoint)   #Get the edges and the center of the box in the 2D coordinates\n",
    "                boxes_gt.append(box_gt)\n",
    "                \n",
    "                dic_names[name]['boxes'].append(box_gt)\n",
    "                dic_names[name]['dds'].append(dd)\n",
    "                dic_names[name]['K'] = kk\n",
    "\n",
    "        return name, path_im, boxes_gt, boxes_3d, dds, kk, keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_categories(cat):\n",
    "    \"\"\"\n",
    "    Choose the categories to extract annotations from\n",
    "    \"\"\"\n",
    "    assert cat in ['person', 'all', 'car', 'cyclist']\n",
    "\n",
    "    if cat == 'person':\n",
    "        categories = ['human.pedestrian']\n",
    "    elif cat == 'all':\n",
    "        categories = ['human.pedestrian', 'vehicle.bicycle', 'vehicle.motorcycle']\n",
    "    elif cat == 'cyclist':\n",
    "        categories = ['vehicle.bicycle']\n",
    "    elif cat == 'car':\n",
    "        categories = ['vehicle.car', 'vehicle.truck']\n",
    "    return categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def factory(dataset, dir_nuscenes):\n",
    "    \"\"\"Define dataset type and split training and validation\"\"\"\n",
    "\n",
    "    assert dataset in ['nuscenes', 'nuscenes_mini', 'nuscenes_teaser']\n",
    "    if dataset == 'nuscenes_mini':\n",
    "        version = 'v1.0-mini'\n",
    "    else:\n",
    "        version = 'v1.0-trainval'\n",
    "\n",
    "    nusc = NuScenes(version=version, dataroot=dir_nuscenes, verbose=True)\n",
    "    scenes = nusc.scene\n",
    "\n",
    "    if dataset == 'nuscenes_teaser':\n",
    "        with open(\"./splits/nuscenes_teaser_scenes.txt\", \"r\") as file:\n",
    "            teaser_scenes = file.read().splitlines()\n",
    "        scenes = [scene for scene in scenes if scene['token'] in teaser_scenes]\n",
    "        with open(\"./splits/split_nuscenes_teaser.json\", \"r\") as file:\n",
    "            dic_split = json.load(file)\n",
    "        split_train = [scene['name'] for scene in scenes if scene['token'] in dic_split['train']]\n",
    "        split_val = [scene['name'] for scene in scenes if scene['token'] in dic_split['val']]\n",
    "    else:\n",
    "        split_scenes = splits.create_splits_scenes()\n",
    "        split_train, split_val = split_scenes['train'], split_scenes['val']\n",
    "\n",
    "    return nusc, scenes, split_train, split_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_keypoints(box):\n",
    "    \n",
    "    xc, yc, zc = box.center\n",
    "    ww, ll, hh, = box.wlh\n",
    "    x,y,z = box.corners().tolist()\n",
    "    \n",
    "    x.append(xc)\n",
    "    y.append(yc)\n",
    "    z.append(zc)\n",
    "\n",
    "    return [x, y, z]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_2d(box, kk):\n",
    "    \"\"\"\n",
    "    Project a 3D bounding box into the pixel frame using the center and the corners of the box\n",
    "    \"\"\"\n",
    "    box_2d = []\n",
    "    xc, yc, zc = box.center\n",
    "    ww, ll, hh, = box.wlh\n",
    "    x,y,z = box.corners().tolist()\n",
    "    \n",
    "    x.append(xc)\n",
    "    y.append(yc)\n",
    "    z.append(zc)\n",
    "    \n",
    "    corners_3d = np.array([[a,b,c] for a,b,c in zip(x,y,z) ])\n",
    "\n",
    "    # Project them and convert into pixel coordinates\n",
    "    for xyz in corners_3d:\n",
    "\n",
    "        xx, yy, zz = np.dot(kk, xyz)\n",
    "        uu = xx / zz\n",
    "        vv = yy / zz\n",
    "        box_2d.append(0 if uu < 0 else 1600 if uu>1600 else uu) #TODO replace the magic numbers\n",
    "        box_2d.append(0 if vv < 0 else 900 if vv>900 else vv)   #TODO replace the magic numbers\n",
    "    #print(box_2d)\n",
    "    return box_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "Loading NuScenes tables for version v1.0-trainval...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "64386 instance,\n",
      "12 sensor,\n",
      "10200 calibrated_sensor,\n",
      "2631083 ego_pose,\n",
      "68 log,\n",
      "850 scene,\n",
      "34149 sample,\n",
      "2631083 sample_data,\n",
      "1166187 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 36.0 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 9.4 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "nusc, scenes, split_train, split_val = factory(dataset,dir_nuscenes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing routine to extract the images informations with the path of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = '/data/bonnesoeur-data/vizualization_monoloco/results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def factory_outputs( images_outputs, output_path, pifpaf_outputs, dic_out=None, kk=None):\n",
    "    \"\"\"Output json files or images according to the choice\"\"\"\n",
    "\n",
    "    if any((xx in args.output_types for xx in ['front', 'bird', 'combined'])):\n",
    "        epistemic = False\n",
    "        if args.n_dropout > 0:\n",
    "            epistemic = True\n",
    "\n",
    "        if dic_out['boxes']:  # Only print in case of detections\n",
    "            printer = Printer(images_outputs[1], output_path, kk, output_types='combined'\n",
    "                              , z_max=33, epistemic=epistemic)\n",
    "            figures, axes = printer.factory_axes()\n",
    "            printer.draw(figures, axes, dic_out, images_outputs[1], draw_box=args.draw_box,\n",
    "                         save=True, show=args.show)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/bonnesoe/semester_project/monoloco/data/models/old/hyp-monoloco-boxes.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-4a2c529df0bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmy_model\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmy_models\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mmonoloco\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMonoLoco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mmonolocos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonoloco\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-97e8ed56e00e>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, device, n_dropout, p_dropout)\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_dropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp_dropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINPUT_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinear_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLINEAR_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# if the model is directly provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/bonnesoe/semester_project/monoloco/data/models/old/hyp-monoloco-boxes.pkl'"
     ]
    }
   ],
   "source": [
    "monolocos = []\n",
    "\n",
    "for my_model in my_models:\n",
    "\n",
    "    monoloco = MonoLoco(my_model, device=torch.device('cuda'))\n",
    "    \n",
    "    monolocos.append(monoloco)\n",
    "        #dic_out = monoloco.post_process(outputs, varss, boxes, keypoints, kk, dic_gt=None)\n",
    "\n",
    "\n",
    "def run():\n",
    "    \n",
    "    cnt_samples=cnt_sd=0\n",
    "    \n",
    "    \n",
    "    for ii, scene in enumerate(scenes):    \n",
    "        current_token = scene['first_sample_token']\n",
    "        print(ii)\n",
    "        \n",
    "        # Select the scenes that you want\n",
    "        if(ii<1 or ii>4):\n",
    "            continue\n",
    "        while not current_token == \"\":\n",
    "            sample_dic = nusc.get('sample', current_token)\n",
    "            cnt_samples += 1\n",
    "            i=0\n",
    "            # Extract all the sample_data tokens for each sample\n",
    "            \n",
    "            for cam in CAMERAS:\n",
    "                sd_token = sample_dic['data'][cam]\n",
    "                cnt_sd += 1\n",
    "\n",
    "                # Extract all the annotations of the person\n",
    "                name, path_img, boxes_gt, boxes_3d, dds, kk, keypoints = extract_from_token(sd_token)\n",
    "                \n",
    "                print(kk)\n",
    "                if keypoints :\n",
    "                    \n",
    "                    inputs = prepare_kps(keypoints)\n",
    "                    \n",
    "                    dic_gt={\n",
    "                        \"boxes\":boxes_gt,\n",
    "                        \"dds\":dds\n",
    "                    }\n",
    "                    \n",
    "                    \n",
    "                    for nn ,nn_type in zip(monolocos,my_models_types):                        \n",
    "                        spacing = \"_\"*150\n",
    "                        print(spacing)\n",
    "                        print(nn_type)\n",
    "                        \n",
    "                        outputs, varss = nn.forward(inputs, kk)\n",
    "                        print(outputs)\n",
    "                        dic_out = nn.post_process(outputs, varss, boxes_gt ,inputs, kk, dic_gt)\n",
    "\n",
    "                        with open(path_img, 'rb') as f:\n",
    "                            pil_image = Image.open(f).convert('RGB')\n",
    "                            image_output=pil_image\n",
    "\n",
    "                        #TODO : modification of printer.py to receive my inpuths as ground truth\n",
    "                        printer = Printer(image_output, output_path, kk, output_types='combined'\n",
    "                              , z_max=40, epistemic=False)\n",
    "                        figures, axes = printer.factory_axes()\n",
    "                        printer.draw(figures, axes, dic_out, image_output, draw_box=True,\n",
    "                                    save=True, show=True)\n",
    "                        plt.show()\n",
    "                \n",
    "\n",
    "                    \n",
    "            current_token = sample_dic['next']    \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import torch\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "%matplotlib inline\n",
    "import datetime\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from nuscenes.nuscenes import NuScenes\n",
    "from nuscenes.utils import splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAMERAS = ('CAM_FRONT', 'CAM_FRONT_LEFT', 'CAM_FRONT_RIGHT', 'CAM_BACK', 'CAM_BACK_LEFT', 'CAM_BACK_RIGHT')\n",
    "dic_jo = {'train': dict(X=[], Y=[], names=[], kps=[], boxes_3d=[], K=[], clst=defaultdict(lambda: defaultdict(list))),\n",
    "          'val': dict(X=[], Y=[], names=[], kps=[], boxes_3d=[], K=[],clst=defaultdict(lambda: defaultdict(list))),\n",
    "          'test': dict(X=[], Y=[], names=[], kps=[], boxes_3d=[], K=[], clst=defaultdict(lambda: defaultdict(list)))\n",
    "          }\n",
    "dic_names = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "dir_nuscenes= '/data/bonnesoeur-data/data/nuscenes/' # Path to your nuscenes folder\n",
    "dataset = \"nuscenes_teaser\"                          # Type of dataset to preprocess\n",
    "dir_out = './json'                                   # Output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! rm -rf $dir_out\n",
    "#! mkdir $dir_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_token(sd_token):\n",
    "\n",
    "        boxes_gt = []\n",
    "        dds = []\n",
    "        boxes_3d = []\n",
    "\n",
    "        keypoints = []\n",
    "        path_im, boxes_obj, kk = nusc.get_sample_data(sd_token, box_vis_level=1)  # At least one corner$\n",
    "\n",
    "    \n",
    "        kk = kk.tolist()\n",
    "        name = os.path.basename(path_im)\n",
    "        for box_obj in boxes_obj:\n",
    "            if box_obj.name[:6] != 'animal':\n",
    "                general_name = box_obj.name.split('.')[0] + '.' + box_obj.name.split('.')[1]\n",
    "            else:\n",
    "                general_name = 'animal'\n",
    "            if general_name in select_categories('car'):\n",
    "                 \n",
    "                keypoint = project_2d(box_obj, kk)\n",
    "                dd = np.linalg.norm(box_obj.center)\n",
    "                bound = prepare_kps([keypoint])[0]\n",
    "                box_gt = [min(bound[0]), min(bound[1]), max(bound[0]), max(bound[1])]\n",
    "                \n",
    "                dds.append(dd)\n",
    "                \n",
    "                \n",
    "                box_3d = box_obj.center.tolist() + box_obj.wlh.tolist()\n",
    "                boxes_3d.append(box_3d)\n",
    "                keypoints.append(keypoint)   #Get the edges and the center of the box in the 2D coordinates\n",
    "                boxes_gt.append(box_gt)\n",
    "                \n",
    "                dic_names[name]['boxes'].append(box_gt)\n",
    "                dic_names[name]['dds'].append(dd)\n",
    "                dic_names[name]['K'] = kk\n",
    "\n",
    "        return name, boxes_gt, boxes_3d, dds, kk, keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_keypoints(box):\n",
    "    \n",
    "    xc, yc, zc = box.center\n",
    "    ww, ll, hh, = box.wlh\n",
    "    x,y,z = box.corners().tolist()\n",
    "    \n",
    "    x.append(xc)\n",
    "    y.append(yc)\n",
    "    z.append(zc)\n",
    "\n",
    "    return [x, y, z]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_2d(box, kk):\n",
    "    \"\"\"\n",
    "    Project a 3D bounding box into the 2D image plane using the center and the corners of the box\n",
    "    \"\"\"\n",
    "    box_2d = []\n",
    "    xc, yc, zc = box.center\n",
    "    ww, ll, hh, = box.wlh\n",
    "    x,y,z = box.corners().tolist()\n",
    "    \n",
    "    x.append(xc)\n",
    "    y.append(yc)\n",
    "    z.append(zc)\n",
    "    \n",
    "    corners_3d = np.array([[a,b,c] for a,b,c in zip(x,y,z) ])\n",
    "\n",
    "    # Project them and convert into pixel coordinates\n",
    "    for xyz in corners_3d:\n",
    "        xx, yy, zz = np.dot(kk, xyz)\n",
    "        uu = xx / zz\n",
    "        vv = yy / zz\n",
    "        box_2d.append(0 if uu < 0 else 1599 if uu>=1600 else uu)\n",
    "        box_2d.append(0 if vv < 0 else 899 if vv>=900 else vv)\n",
    "    #print(box_2d)\n",
    "    return box_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_cluster(dic_jo, phase, xx, dd, kps):\n",
    "    \"\"\"Append the annotation based on its distance\"\"\"\n",
    "\n",
    "    if dd <= 10:\n",
    "        dic_jo[phase]['clst']['10']['kps'].append(kps)\n",
    "        dic_jo[phase]['clst']['10']['X'].append(xx)\n",
    "        dic_jo[phase]['clst']['10']['Y'].append([dd])\n",
    "\n",
    "   \n",
    "        dic_jo[phase]['clst']['20']['kps'].append(kps)\n",
    "        dic_jo[phase]['clst']['20']['X'].append(xx)\n",
    "        dic_jo[phase]['clst']['20']['Y'].append([dd])\n",
    "\n",
    "    elif dd <= 30:\n",
    "        dic_jo[phase]['clst']['30']['kps'].append(kps)\n",
    "        dic_jo[phase]['clst']['30']['X'].append(xx)\n",
    "        dic_jo[phase]['clst']['30']['Y'].append([dd])\n",
    "\n",
    "    else:\n",
    "        dic_jo[phase]['clst']['>30']['kps'].append(kps)\n",
    "        dic_jo[phase]['clst']['>30']['X'].append(xx)\n",
    "        dic_jo[phase]['clst']['>30']['Y'].append([dd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_categories(cat):\n",
    "    \"\"\"\n",
    "    Choose the categories to extract annotations from\n",
    "    \"\"\"\n",
    "    assert cat in ['person', 'all', 'car', 'cyclist']\n",
    "\n",
    "    if cat == 'person':\n",
    "        categories = ['human.pedestrian']\n",
    "    elif cat == 'all':\n",
    "        categories = ['human.pedestrian', 'vehicle.bicycle', 'vehicle.motorcycle']\n",
    "    elif cat == 'cyclist':\n",
    "        categories = ['vehicle.bicycle']\n",
    "    elif cat == 'car':\n",
    "        categories = ['vehicle.car', 'vehicle.truck']\n",
    "    return categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "        \"\"\"\n",
    "        Prepare arrays for training\n",
    "        \"\"\"\n",
    "        cnt_scenes = cnt_samples = cnt_sd = cnt_ann = 0\n",
    "        start = time.time()\n",
    "        for ii, scene in enumerate(scenes):\n",
    "            end_scene = time.time()\n",
    "            current_token = scene['first_sample_token']\n",
    "            cnt_scenes += 1\n",
    "            time_left = str((end_scene - start_scene) / 60 * (len(scenes) - ii))[:4] if ii != 0 else \"NaN\"\n",
    "\n",
    "            sys.stdout.write('\\r' + 'Elaborating scene {}, remaining time {} minutes'\n",
    "                             .format(cnt_scenes, time_left) + '\\t\\n')\n",
    "            start_scene = time.time()\n",
    "            if scene['name'] in split_train:\n",
    "                phase = 'train'\n",
    "            elif scene['name'] in split_val:\n",
    "                phase = 'val'\n",
    "            else:\n",
    "                print(\"phase name not in training or validation split\")\n",
    "                continue\n",
    "\n",
    "            while not current_token == \"\":\n",
    "                sample_dic = nusc.get('sample', current_token)\n",
    "                cnt_samples += 1\n",
    "\n",
    "                # Extract all the sample_data tokens for each sample\n",
    "                for cam in CAMERAS:\n",
    "                    sd_token = sample_dic['data'][cam]\n",
    "                    cnt_sd += 1\n",
    "\n",
    "                    # Extract all the annotations of the person\n",
    "                    #? kk intrinsic camera parametersmatrix\n",
    "                    name, boxes_gt, boxes_3d, dds, kk, keypoints = extract_from_token(sd_token)\n",
    "                    \n",
    "                    if keypoints:\n",
    "                        keypoints = prepare_kps(keypoints) #Convert an tensor of 18 to a tensor of 3*9 (the last column is neglected)\n",
    "                        inputs = preprocess_monoloco(keypoints, kk).tolist()\n",
    "                        for box_gt, box_3d, keypoint, dd, my_input  in zip(boxes_gt,boxes_3d, keypoints,dds, inputs):\n",
    "\n",
    "                            dic_jo[phase]['kps'].append(keypoint) \n",
    "                            #print(keypoint)\n",
    "                            #print(box_gt)\n",
    "                            if len(my_input)!=18:\n",
    "                                continue;\n",
    "                            dic_jo[phase]['X'].append(my_input)\n",
    "                            dic_jo[phase]['Y'].append(dd)  # Trick to make it (nn,1)\n",
    "                            dic_jo[phase]['names'].append(name)  # One image name for each annotation\n",
    "                            dic_jo[phase]['boxes_3d'].append(box_3d)\n",
    "                            dic_jo[phase]['K'].append(kk)\n",
    "                            append_cluster(dic_jo, phase, my_input ,dd, keypoint)\n",
    "                            cnt_ann += 1\n",
    "                            sys.stdout.write('\\r' + 'Saved annotations {}'.format(cnt_ann) + '\\t')\n",
    "                        \n",
    "                        \n",
    "                current_token = sample_dic['next']\n",
    "\n",
    "        with open(os.path.join(path_joints), 'w') as f:\n",
    "            json.dump(dic_jo, f)\n",
    "        with open(os.path.join(path_names), 'w') as f:\n",
    "            json.dump(dic_names, f)\n",
    "        end = time.time()\n",
    "\n",
    "        print(\"\\nSaved {} annotations for {} samples in {} scenes. Total time: {:.1f} minutes\"\n",
    "              .format(cnt_ann, cnt_samples, cnt_scenes, (end-start)/60))\n",
    "        print(\"\\nOutput files:\\n{}\\n{}\\n\".format(path_names, path_joints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_monoloco(keypoints, kk):\n",
    "\n",
    "    \"\"\" Preprocess batches of inputs\n",
    "    keypoints = torch tensors of (m, 3, 14)  or list [3,14]\n",
    "    Outputs =  torch tensors of (m, 34) in meters normalized (z=1) and zero-centered using the center of the box\n",
    "    \"\"\"\n",
    "    if isinstance(keypoints, list):\n",
    "        keypoints = torch.tensor(keypoints)\n",
    "    if isinstance(kk, list):\n",
    "        kk = torch.tensor(kk)\n",
    "    # Projection in normalized image coordinates and zero-center with the center of the bounding box\n",
    "    uv_center = get_keypoints(keypoints, mode='center')\n",
    "    xy1_center = pixel_to_camera(uv_center, kk, 10)\n",
    "    xy1_all = pixel_to_camera(keypoints[:, 0:2, :], kk, 10)\n",
    "    kps_norm = xy1_all - xy1_center.unsqueeze(1)  # (m, 17, 3) - (m, 1, 3)\n",
    "    kps_out = kps_norm[:, :, 0:2].reshape(kps_norm.size()[0], -1)  # no contiguous for view\n",
    "    return kps_out \n",
    "\n",
    "def prepare_kps(kps_in):\n",
    "    \"\"\"Convert from a list of 18 to a list of 3, 9\"\"\"\n",
    "\n",
    "    kps = []\n",
    "    for kp_in in kps_in :\n",
    "        assert len(kp_in) % 2 == 0, \"keypoints expected as a multiple of 2\"\n",
    "        xxs = kp_in[0:][::2]\n",
    "        yys = kp_in[1:][::2]  # from offset 1 every 2\n",
    "        ccs = [1]*len(kp_in[1:][::2])\n",
    "        kps.append([xxs, yys, ccs])\n",
    "    return kps\n",
    "\n",
    "\n",
    "def pixel_to_camera(uv_tensor, kk, z_met):\n",
    "    \"\"\"\n",
    "    Convert a tensor in pixel coordinate to absolute camera coordinates\n",
    "    It accepts lists or torch/numpy tensors of (m, 2) or (m, x, 2)\n",
    "    where x is the number of keypoints\n",
    "    \"\"\"\n",
    "    if isinstance(uv_tensor, (list, np.ndarray)):\n",
    "        uv_tensor = torch.tensor(uv_tensor)\n",
    "    if isinstance(kk, list):\n",
    "        kk = torch.tensor(kk)\n",
    "    if uv_tensor.size()[-1] != 2:\n",
    "        uv_tensor = uv_tensor.permute(0, 2, 1)  # permute to have 2 as last dim to be padded\n",
    "        assert uv_tensor.size()[-1] == 2, \"Tensor size not recognized\"\n",
    "    uv_padded = F.pad(uv_tensor, pad=(0, 1), mode=\"constant\", value=1)  # pad only last-dim below with value 1\n",
    "\n",
    "    kk_1 = torch.inverse(kk)\n",
    "    xyz_met_norm = torch.matmul(uv_padded, kk_1.t())  # More general than torch.mm\n",
    "    xyz_met = xyz_met_norm * z_met\n",
    "\n",
    "    return xyz_met\n",
    "\n",
    "\n",
    "def get_keypoints(keypoints, mode):\n",
    "    \"\"\"\n",
    "    Extract center, shoulder or hip points of a keypoint\n",
    "    Input --> list or torch/numpy tensor [(m, 3, 9) or (3, 9)]\n",
    "    Output --> torch.tensor [(m, 2)]\n",
    "    \"\"\"\n",
    "    if isinstance(keypoints, (list, np.ndarray)):\n",
    "        keypoints = torch.tensor(keypoints)\n",
    "    if len(keypoints.size()) == 2:  # add batch dim\n",
    "        keypoints = keypoints.unsqueeze(0)\n",
    "        \n",
    "    assert len(keypoints.size()) == 3 and keypoints.size()[1] == 3, \"tensor dimensions not recognized\"\n",
    "    assert mode in ['center', 'bottom', 'head', 'shoulder', 'hip', 'ankle']\n",
    "\n",
    "    kps_in = keypoints[:, 0:2, :]  # (m, 2, 9)\n",
    "    if mode == 'center':\n",
    "        kps_max, _ = kps_in.max(2)  # returns value, indices\n",
    "        kps_min, _ = kps_in.min(2)\n",
    "        kps_out = (kps_max - kps_min) / 2 + kps_min   # (m, 2) as keepdims is False\n",
    "\n",
    "    elif mode == 'bottom':  # bottom center for kitti evaluation\n",
    "        kps_max, _ = kps_in.max(2)\n",
    "        kps_min, _ = kps_in.min(2)\n",
    "        kps_out_x = (kps_max[:, 0:1] - kps_min[:, 0:1]) / 2 + kps_min[:, 0:1]\n",
    "        kps_out_y = kps_max[:, 1:2]\n",
    "        kps_out = torch.cat((kps_out_x, kps_out_y), -1)\n",
    "\n",
    "    elif mode == 'head':\n",
    "        kps_out = kps_in[:, :, 0:5].mean(2)\n",
    "\n",
    "    elif mode == 'shoulder':\n",
    "        kps_out = kps_in[:, :, 5:7].mean(2)\n",
    "\n",
    "    elif mode == 'hip':\n",
    "        kps_out = kps_in[:, :, 11:13].mean(2)\n",
    "\n",
    "    elif mode == 'ankle':\n",
    "        kps_out = kps_in[:, :, 15:17].mean(2)\n",
    "\n",
    "    return kps_out  # (m, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def factory(dataset, dir_nuscenes):\n",
    "    \"\"\"Define dataset type and split training and validation\"\"\"\n",
    "\n",
    "    assert dataset in ['nuscenes', 'nuscenes_mini', 'nuscenes_teaser']\n",
    "    if dataset == 'nuscenes_mini':\n",
    "        version = 'v1.0-mini'\n",
    "    else:\n",
    "        version = 'v1.0-trainval'\n",
    "\n",
    "    nusc = NuScenes(version=version, dataroot=dir_nuscenes, verbose=True)\n",
    "    scenes = nusc.scene\n",
    "\n",
    "    if dataset == 'nuscenes_teaser':\n",
    "        with open(\"./splits/nuscenes_teaser_scenes.txt\", \"r\") as file:\n",
    "            teaser_scenes = file.read().splitlines()\n",
    "        scenes = [scene for scene in scenes if scene['token'] in teaser_scenes]\n",
    "        with open(\"./splits/split_nuscenes_teaser.json\", \"r\") as file:\n",
    "            dic_split = json.load(file)\n",
    "        split_train = [scene['name'] for scene in scenes if scene['token'] in dic_split['train']]\n",
    "        split_val = [scene['name'] for scene in scenes if scene['token'] in dic_split['val']]\n",
    "    else:\n",
    "        split_scenes = splits.create_splits_scenes()\n",
    "        split_train, split_val = split_scenes['train'], split_scenes['val']\n",
    "\n",
    "    return nusc, scenes, split_train, split_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "Loading NuScenes tables for version v1.0-trainval...\n",
      "23 category,\n",
      "8 attribute,\n",
      "4 visibility,\n",
      "64386 instance,\n",
      "12 sensor,\n",
      "10200 calibrated_sensor,\n",
      "2631083 ego_pose,\n",
      "68 log,\n",
      "850 scene,\n",
      "34149 sample,\n",
      "2631083 sample_data,\n",
      "1166187 sample_annotation,\n",
      "4 map,\n",
      "Done loading in 35.2 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 11.6 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "now = datetime.datetime.now()\n",
    "now_time = now.strftime(\"%Y%m%d-%H%M\")[2:]\n",
    "path_joints = os.path.join(dir_out, 'joints-' + dataset + '-' + now_time + '.json')\n",
    "path_names = os.path.join(dir_out, 'names-' + dataset + '-' + now_time + '.json')\n",
    "\n",
    "nusc, scenes, split_train, split_val = factory(dataset,dir_nuscenes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elaborating scene 1, remaining time NaN minutes\t\n",
      "Elaborating scene 2, remaining time 1.71 minutes\t\n",
      "Elaborating scene 3, remaining time 3.56 minutes\t\n",
      "Elaborating scene 4, remaining time 9.32 minutes\t\n",
      "Elaborating scene 5, remaining time 2.99 minutes\t\n",
      "Elaborating scene 6, remaining time 5.45 minutes\t\n",
      "Elaborating scene 7, remaining time 2.40 minutes\t\n",
      "Elaborating scene 8, remaining time 1.86 minutes\t\n",
      "Elaborating scene 9, remaining time 1.17 minutes\t\n",
      "Elaborating scene 10, remaining time 1.16 minutes\t\n",
      "Elaborating scene 11, remaining time 2.27 minutes\t\n",
      "Elaborating scene 12, remaining time 0.33 minutes\t\n",
      "Elaborating scene 13, remaining time 1.30 minutes\t\n",
      "phase name not in training or validation split\n",
      "Elaborating scene 14, remaining time 4.00 minutes\t\n",
      "phase name not in training or validation split\n",
      "Elaborating scene 15, remaining time 2.54 minutes\t\n",
      "Elaborating scene 16, remaining time 2.42 minutes\t\n",
      "Elaborating scene 17, remaining time 5.96 minutes\t\n",
      "Elaborating scene 18, remaining time 7.43 minutes\t\n",
      "Elaborating scene 19, remaining time 5.69 minutes\t\n",
      "Elaborating scene 20, remaining time 1.68 minutes\t\n",
      "Elaborating scene 21, remaining time 4.74 minutes\t\n",
      "Elaborating scene 22, remaining time 4.46 minutes\t\n",
      "Elaborating scene 23, remaining time 1.26 minutes\t\n",
      "Elaborating scene 24, remaining time 1.85 minutes\t\n",
      "Elaborating scene 25, remaining time 2.33 minutes\t\n",
      "Elaborating scene 26, remaining time 1.94 minutes\t\n",
      "Elaborating scene 27, remaining time 1.83 minutes\t\n",
      "Elaborating scene 28, remaining time 2.78 minutes\t\n",
      "Elaborating scene 29, remaining time 3.82 minutes\t\n",
      "Elaborating scene 30, remaining time 1.67 minutes\t\n",
      "Elaborating scene 31, remaining time 1.23 minutes\t\n",
      "Elaborating scene 32, remaining time 0.38 minutes\t\n",
      "Elaborating scene 33, remaining time 0.45 minutes\t\n",
      "phase name not in training or validation split\n",
      "Elaborating scene 34, remaining time 2.26 minutes\t\n",
      "Elaborating scene 35, remaining time 2.09 minutes\t\n",
      "Elaborating scene 36, remaining time 2.82 minutes\t\n",
      "Elaborating scene 37, remaining time 0.88 minutes\t\n",
      "Elaborating scene 38, remaining time 1.57 minutes\t\n",
      "phase name not in training or validation split\n",
      "Elaborating scene 39, remaining time 2.17 minutes\t\n",
      "Elaborating scene 40, remaining time 3.32 minutes\t\n",
      "Elaborating scene 41, remaining time 0.84 minutes\t\n",
      "Elaborating scene 42, remaining time 3.07 minutes\t\n",
      "Elaborating scene 43, remaining time 3.07 minutes\t\n",
      "Elaborating scene 44, remaining time 3.30 minutes\t\n",
      "Elaborating scene 45, remaining time 1.56 minutes\t\n",
      "Elaborating scene 46, remaining time 2.31 minutes\t\n",
      "Elaborating scene 47, remaining time 0.80 minutes\t\n",
      "Elaborating scene 48, remaining time 1.11 minutes\t\n",
      "Elaborating scene 49, remaining time 0.60 minutes\t\n",
      "Elaborating scene 50, remaining time 1.23 minutes\t\n",
      "Elaborating scene 51, remaining time 1.00 minutes\t\n",
      "Elaborating scene 52, remaining time 0.82 minutes\t\n",
      "Elaborating scene 53, remaining time 0.56 minutes\t\n",
      "Elaborating scene 54, remaining time 1.11 minutes\t\n",
      "Elaborating scene 55, remaining time 1.94 minutes\t\n",
      "Elaborating scene 56, remaining time 0.63 minutes\t\n",
      "Elaborating scene 57, remaining time 0.43 minutes\t\n",
      "Elaborating scene 58, remaining time 5.16 minutes\t\n",
      "Elaborating scene 59, remaining time 1.47 minutes\t\n",
      "Elaborating scene 60, remaining time 1.85 minutes\t\n",
      "Elaborating scene 61, remaining time 0.89 minutes\t\n",
      "Elaborating scene 62, remaining time 1.21 minutes\t\n",
      "Elaborating scene 63, remaining time 2.02 minutes\t\n",
      "Elaborating scene 64, remaining time 1.71 minutes\t\n",
      "Elaborating scene 65, remaining time 0.40 minutes\t\n",
      "phase name not in training or validation split\n",
      "Elaborating scene 66, remaining time 1.10 minutes\t\n",
      "Elaborating scene 67, remaining time 0.28 minutes\t\n",
      "Elaborating scene 68, remaining time 1.13 minutes\t\n",
      "Elaborating scene 69, remaining time 0.66 minutes\t\n",
      "Elaborating scene 70, remaining time 0.55 minutes\t\n",
      "Elaborating scene 71, remaining time 1.28 minutes\t\n",
      "Elaborating scene 72, remaining time 0.85 minutes\t\n",
      "Elaborating scene 73, remaining time 1.18 minutes\t\n",
      "Elaborating scene 74, remaining time 1.33 minutes\t\n",
      "Elaborating scene 75, remaining time 0.81 minutes\t\n",
      "phase name not in training or validation split\n",
      "Elaborating scene 76, remaining time 6.99 minutes\t\n",
      "Elaborating scene 77, remaining time 0.97 minutes\t\n",
      "Elaborating scene 78, remaining time 0.66 minutes\t\n",
      "Elaborating scene 79, remaining time 0.57 minutes\t\n",
      "Elaborating scene 80, remaining time 0.16 minutes\t\n",
      "Elaborating scene 81, remaining time 0.28 minutes\t\n",
      "Elaborating scene 82, remaining time 0.85 minutes\t\n",
      "Elaborating scene 83, remaining time 0.90 minutes\t\n",
      "Elaborating scene 84, remaining time 0.60 minutes\t\n",
      "phase name not in training or validation split\n",
      "Elaborating scene 85, remaining time 3.89 minutes\t\n",
      "Elaborating scene 86, remaining time 0.44 minutes\t\n",
      "Elaborating scene 87, remaining time 0.39 minutes\t\n",
      "Elaborating scene 88, remaining time 0.36 minutes\t\n",
      "Elaborating scene 89, remaining time 0.39 minutes\t\n",
      "Elaborating scene 90, remaining time 0.27 minutes\t\n",
      "Elaborating scene 91, remaining time 0.18 minutes\t\n",
      "Elaborating scene 92, remaining time 0.09 minutes\t\n",
      "Elaborating scene 93, remaining time 0.20 minutes\t\n",
      "Elaborating scene 94, remaining time 0.10 minutes\t\n",
      "phase name not in training or validation split\n",
      "Elaborating scene 95, remaining time 3.33 minutes\t\n",
      "Saved annotations 51576\t\n",
      "Saved 51576 annotations for 3461 samples in 95 scenes. Total time: 3.4 minutes\n",
      "\n",
      "Output files:\n",
      "/data/bonnesoeur-data/monoloco/box_nuscenes/json_car_only/names-nuscenes_teaser-200106-2018.json\n",
      "/data/bonnesoeur-data/monoloco/box_nuscenes/json_car_only/joints-nuscenes_teaser-200106-2018.json\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
